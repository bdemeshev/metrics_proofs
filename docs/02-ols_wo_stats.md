# МНК без статистических предпосылок {#ols_wo_stats}





Теорема. 

Рассмотрим модель $y=X\beta + u$. 
С помощью МНК мы получаем оценки коэффициентов $\hat\beta$, прогнозы $\hat y$ и остатки $\hat u$. Выполним кросс-валидацию, то есть по очереди будем оценивать модель без первого наблюдения,
потом без второго, потом без третьего и так далее $n$ раз. 
Каждый раз будем прогнозировать значение зависимой переменной для выкинутого наблюдения и получим
вектор кросс-валидационных прогнозов $\hat y^{CV}$ и вектор ошибок прогнозов $\hat u^{CV}$.

При этом окажется, что 
\[
\hat u_i = (1 - H_{ii}) \cdot \hat u_i^{CV},
\]
где $H$ — матрица-шляпница, $H = X(X'X)^{-1}X'$.

Доказательство. 

Оценим модель без последнего наблюдения, $\hat y^{-} = X^{-} \hat\beta^{-}$. 

Создадим вектор $y^{*}$, который будет отличаться от $y$ только последним, $n$-м элементом:
вместо настоящего $y_n$ там будет стоять прогноз по модели без последнего наблюдения $\hat y^{-}_n$.

Раз уж мы добавили новую точку лежащую ровно на выборочной регрессии, то при оценки модели
$\hat y^* = X \hat \beta^*$ мы получим в точности старые оценки $\hat \beta^* = \hat \beta^-$. 
Следовательно, и прогнозы эти две модели дают одинаковые, $\hat y_i^* = \hat y_i^-$.

А теперь посмотрим на последний элемент вектора $v = H (y^* - y)$.

С одной стороны, он равен последней строке матрицы $H$ умножить на вектор $(y^* - y)$. 
В векторе $(y^* - y)$ только последний элемент ненулевой, поэтому $v_n = H_{nn} (\hat y^{-}_n - y_n)$.

С другой стороны, мы можем раскрыть скобки, и заметить, что $v = Hy^* - Hy$. 
И окажется, что $v_n = \hat y_n^* - \hat y_n = \hat y_n^- - \hat y_n$.

Отсюда
\[
 \hat y_n^- - \hat y_n = H_{nn} (\hat y_n^- - y_n)
\]

Приводим подобные слагаемые и добавляем слева и справа $y_n$, получаем как раз то, что нужно:
\[
y_n - \hat y_n   = (1 - H_{ii}) (y_n - \hat y_n^- )
\]



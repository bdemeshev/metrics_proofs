[
["index.html", "A Minimal Book Example Chapter 1 Prerequisites", " A Minimal Book Example Yihui Xie 2019-03-12 Chapter 1 Prerequisites This is a sample book written in Markdown. You can use anything that Pandoc’s Markdown supports, e.g., a math equation \\(a^2 + b^2 = c^2\\). The bookdown package can be installed from CRAN or Github: install.packages(&quot;bookdown&quot;) # or the development version # devtools::install_github(&quot;rstudio/bookdown&quot;) Remember each Rmd file contains one and only one chapter, and a chapter is defined by the first-level heading #. To compile this example to PDF, you need XeLaTeX. You are recommended to install TinyTeX (which includes XeLaTeX): https://yihui.name/tinytex/. "],
["intro.html", "Chapter 2 Introduction", " Chapter 2 Introduction Текущий план: Метод главных компонент и кластеризация МНК без статистических предпосылок МНК и дисперсия МНК и нормальные ошибки МНК и большие выборки Гетероскедастичность Эндогенность Метод максимального правдоподобия Логит и пробит Деревья и леса Временные ряды Немного панельных данных You can label chapter and section titles using {#label} after them, e.g., we can reference Chapter 2. If you do not manually label them, there will be automatic labels anyway, e.g., Chapter ??. Figures and tables with captions will be placed in figure and table environments, respectively. par(mar = c(4, 4, .1, .1)) plot(pressure, type = &#39;b&#39;, pch = 19) Figure 2.1: Here is a nice figure! Reference a figure by its code chunk label with the fig: prefix, e.g., see Figure 2.1. Similarly, you can reference tables generated from knitr::kable(), e.g., see Table 2.1. knitr::kable( head(iris, 20), caption = &#39;Here is a nice table!&#39;, booktabs = TRUE ) Table 2.1: Here is a nice table! Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.6 3.4 1.4 0.3 setosa 5.0 3.4 1.5 0.2 setosa 4.4 2.9 1.4 0.2 setosa 4.9 3.1 1.5 0.1 setosa 5.4 3.7 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 4.8 3.0 1.4 0.1 setosa 4.3 3.0 1.1 0.1 setosa 5.8 4.0 1.2 0.2 setosa 5.7 4.4 1.5 0.4 setosa 5.4 3.9 1.3 0.4 setosa 5.1 3.5 1.4 0.3 setosa 5.7 3.8 1.7 0.3 setosa 5.1 3.8 1.5 0.3 setosa You can write citations, too. For example, we are using the bookdown package (Xie 2018) in this sample book, which was built on top of R Markdown and knitr (Xie 2015). References "],
["ols-wo-stats.html", "Chapter 3 МНК без статистических предпосылок", " Chapter 3 МНК без статистических предпосылок Теорема. Рассмотрим модель \\(y=X\\beta + u\\). С помощью МНК мы получаем оценки коэффициентов \\(\\hat\\beta\\), прогнозы \\(\\hat y\\) и остатки \\(\\hat u\\). Выполним кросс-валидацию, то есть по очереди будем оценивать модель без первого наблюдения, потом без второго, потом без третьего и так далее \\(n\\) раз. Каждый раз будем прогнозировать значение зависимой переменной для выкинутого наблюдения и получим вектор кросс-валидационных прогнозов \\(\\hat y^{CV}\\) и вектор ошибок прогнозов \\(\\hat u^{CV}\\). При этом окажется, что \\[ \\hat u_i = (1 - H_{ii}) \\cdot \\hat u_i^{CV}, \\] где \\(H\\) — матрица-шляпница, \\(H = X(X&#39;X)^{-1}X&#39;\\). Доказательство. Оценим модель без последнего наблюдения, \\(\\hat y^{-} = X^{-} \\hat\\beta^{-}\\). Создадим вектор \\(y^{*}\\), который будет отличаться от \\(y\\) только последним, \\(n\\)-м элементом: вместо настоящего \\(y_n\\) там будет стоять прогноз по модели без последнего наблюдения \\(\\hat y^{-}_n\\). Раз уж мы добавили новую точку лежащую ровно на выборочной регрессии, то при оценки модели \\(\\hat y^* = X \\hat \\beta^*\\) мы получим в точности старые оценки \\(\\hat \\beta^* = \\hat \\beta^-\\). Следовательно, и прогнозы эти две модели дают одинаковые, \\(\\hat y_i^* = \\hat y_i^-\\). А теперь посмотрим на последний элемент вектора \\(v = H (y - y^*)\\). С одной стороны, он равен последней строке матрицы \\(H\\) умножить на вектор \\((y- y^*)\\). В векторе \\((y-y^*)\\) только последний элемент ненулевой, поэтому \\(v_n = H_{nn} (y_n - \\hat y^{-}_n)\\). С другой стороны, мы можем раскрыть скобки, и заметить, что \\(v = Hy - Hy^*\\). И окажется, что \\(v_n = \\hat y_n - \\hat y_n^* = \\hat y_n - \\hat y_n^-\\). Отсюда \\[ \\hat y_n - \\hat y_n^- = H_{nn} (y_n - \\hat y^{-}_n) \\] Приводим подобные слагаемые и вычитаем слева и справа \\(y_n\\), получаем как раз то, что нужно: \\[ \\hat y_n - y_n = (1 - H_{ii}) (\\hat y_n^- - y_n) \\] "]
]

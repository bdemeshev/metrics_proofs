\documentclass[11pt,russian,]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
    \setmainfont[]{Linux Libertine O}
    \setsansfont[]{Linux Libertine O}
    \setmonofont[Mapping=tex-ansi]{Linux Libertine O}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={Эконометрика: доказательства},
            pdfauthor={Борис Демешев},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[shorthands=off,main=russian]{babel}
\else
  \usepackage{polyglossia}
  \setmainlanguage[]{russian}
\fi
\usepackage{natbib}
\bibliographystyle{apalike}
\usepackage{longtable,booktabs}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{Эконометрика: доказательства}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
    \author{Борис Демешев}
    \preauthor{\centering\large\emph}
  \postauthor{\par}
      \predate{\centering\large\emph}
  \postdate{\par}
    \date{2019-03-13}

\usepackage{booktabs}
\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother


% why do we need \newfontfamily:
% http://tex.stackexchange.com/questions/91507/
\newfontfamily{\cyrillicfonttt}{Linux Libertine O}
\newfontfamily{\cyrillicfont}{Linux Libertine O}
\newfontfamily{\cyrillicfontsf}{Linux Libertine O}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{intro}{%
\chapter{Введение}\label{intro}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Метод главных компонент и кластеризация
\item
  МНК без статистических предпосылок
\item
  МНК и дисперсия
\item
  МНК и нормальные ошибки
\item
  МНК и большие выборки
\item
  Гетероскедастичность
\item
  Эндогенность
\item
  Метод максимального правдоподобия
\item
  Логит и пробит
\item
  Деревья и леса
\item
  Временные ряды
\item
  Немного панельных данных
\end{enumerate}

\hypertarget{pca}{%
\chapter{Метод главных компонент}\label{pca}}

\hypertarget{ols_wo_stats}{%
\chapter{МНК без статистических предпосылок}\label{ols_wo_stats}}

Определение.

Кросс-валидация с поочередным выкидыванием отдельных наблюдений.

Leave one out cross validation.

Рассмотрим модель \(y=X\beta + u\).

Оценим модель без первого наблюдения. Получим МНК-оценки \(\hat\beta^{(-1)}\).
С помощью этих оценок спрогнозируем первое наблюдение, получим прогноз \(\hat y_1^{CV}\) и ошибку прогноза \(\hat u_1^{CV}\).

Вернём первое наблюдение в выборку и удалим второе наблюдение. Получим МНК-оценки \(\hat\beta^{(-2)}\).
С помощью этих оценок спрогнозируем второе наблюдение, получим прогноз \(\hat y_2^{CV}\) и ошибку прогноза \(\hat u_2^{CV}\).

Поступим так с каждым наблюдением. На выходе получим вектор кросс-валидационных прогнозов \(\hat y^{CV}\) и вектор кросс-валидационных ошибок прогнозов \(\hat u^{CV}\).

Теорема.

Если модель \(y=X\beta + u\) оценивается с помощью МНК и проводится кросс-валидации с поочередным выкидыванием отдельных наблюдений, то:

\[
\hat u_i = (1 - H_{ii}) \cdot \hat u_i^{CV},
\]
где \(H\) --- матрица-шляпница \(H = X(X'X)^{-1}X'\), \(\hat u\) --- остатки регрессии, а \(\hat u^{CV}\) --- кросс-валидационные ошибки прогнозов.

Доказательство.

Оценим модель без последнего наблюдения, \(\hat y^{-} = X^{-} \hat\beta^{-}\).

Создадим вектор \(y^{*}\), который будет отличаться от \(y\) только последним, \(n\)-м элементом:
вместо настоящего \(y_n\) там будет стоять прогноз по модели без последнего наблюдения \(\hat y^{-}_n\).

Раз уж мы добавили новую точку лежащую ровно на выборочной регрессии, то при оценки модели
\(\hat y^* = X \hat \beta^*\) мы получим в точности старые оценки \(\hat \beta^* = \hat \beta^-\).
Следовательно, и прогнозы эти две модели дают одинаковые, \(\hat y_i^* = \hat y_i^-\).

А теперь посмотрим на последний элемент вектора \(v = H (y^* - y)\).

С одной стороны, он равен последней строке матрицы \(H\) умножить на вектор \((y^* - y)\).
В векторе \((y^* - y)\) только последний элемент ненулевой, поэтому \(v_n = H_{nn} (\hat y^{-}_n - y_n)\).

С другой стороны, мы можем раскрыть скобки, и заметить, что \(v = Hy^* - Hy\).
И окажется, что \(v_n = \hat y_n^* - \hat y_n = \hat y_n^- - \hat y_n\).

Отсюда
\[
 \hat y_n^- - \hat y_n = H_{nn} (\hat y_n^- - y_n)
\]

Приводим подобные слагаемые и добавляем слева и справа \(y_n\), получаем как раз то, что нужно:
\[
y_n - \hat y_n   = (1 - H_{ii}) (y_n - \hat y_n^- )
\]

\hypertarget{ols_variance}{%
\chapter{МНК и дисперсия}\label{ols_variance}}

\hypertarget{divine_regression}{%
\section{Задача о божественной регрессии}\label{divine_regression}}

Будем работать не с выборкой, как простые смертные, а подобно богам Олимпа подумаем о случайных величинах!

Пусть \(r\) --- случайная величина, которую мы хотим приблизить с помощью линейной комбинации случайных величин \(\beta_1 s_1 + \ldots + \beta_k s_k\).

А именно, мы хотим минимизировать
\[
E((r - \beta's)^2) \to \min_{\beta} 
\]

Если среди случайных величин \(s_1\), \ldots, \(s_k\) есть константа, то можно эту величину-константу убрать и вместо этого минимизировать функцию
\[
Var(r-\beta's) \to \min_{\beta}
\]

Найдите оптимальный вектор \(\beta\), достигаемый минимум функции.

Решение:

Теорема Гаусса-Маркова

Модель \(y = X\beta + u\) оценивается с помощью МНК.

TODO: дописать оставшуюся часть теоремы и доказательства

Доказательство эффективности МНК-оценок.

Эффективность МНК-оценок --- это реинкарнация теоремы Пифагора.
Мы докажем, что дисперсия МНК-оценки --- это квадрат длины катета,
дисперсия альтернативной несмещённой оценки --- квадрат длины гипотенузы.

Для примера рассмотрим первый коэффициент бета. Доказательство не меняется ни капли, если рассмотреть произвольную линейную комбинацию коэффициентов бета.
У нас есть две оценки, \(\hat\beta_1\) и \(\hat\beta_1^{alt}\). Обе они линейны по \(y\),
следовательно, \(\hat\beta_1 = a'y\) и \(\hat\beta_1^{alt} = a'_{alt} y\).

Замечаем, что \(Var(\hat\beta_1) = \sigma^2 a'a\), и \(Var(\hat\beta_1^{alt}) = \sigma^2 a_{alt}'a_{alt}\). То есть дисперсии пропорциональны квадратм длин векторов \(a\) и \(a^{alt}\).
Осталось доказать, что вектор \(a\) не длиннее вектора \(a^{alt}\) :)

Для этого мы докажем, что \(a^{alt}\) --- это гипотенуза, а \(a\) --- катет. То есть нужно доказать, что вектор \(a - a^{alt}\) перпендикулярен вектору \(a\).

Разобъём доказательство перпендикулярности \(a\) и \(a-a^{alt}\) на два шага:

Шаг 1. Вектор \(a - a^{alt}\) перпендикулярен любому столбцу матрицы \(X\).

Шаг 2. Вектор \(a\) является линейной комбинацией столбцов матрицы \(X\).

TODO: здесь картинка!

Приступаем к шагу 1. Обе оценки несмещённые, поэтому для любых \(\beta\) должно выполняться:

\[
E(\hat\beta_1) = E(\hat\beta_1^{alt})
\]

Переносим всё в левую сторону:

\[
E((a' - a'_{alt})(X\beta + u)) = 0
\]

Получаем, что для любых \(\beta\) должно быть выполнено условие:

\[
(a - a_{alt})'X\beta = 0
\]

Но это возможно только если вектор \((a - a_{alt})'X\) равен нулю.
Следовательно, вектор \((a - a_{alt})\) перпендикулярен любому столбцу \(X\).

Приступаем к шагу 2.

Вспоминаем, что \(\hat \beta = (X'X)^{-1}X'y\). Следовательно, нужная строка весов \(a'\) ---
это первая строка в матрице \((X'X)^{-1}X'\).
Замечаем, что выражение имеет вид \(A \cdot X'\).

Вспоминаем из линейной алгебры, что при умножении матриц \(AB\) получается матрица \(C\),
на которую можно взглянуть двумя способами.
Можно считать, что \(C\) --- это разные линейные комбинации столбцов левой матрицы \(A\),
а можно считать, что \(C\) --- это разные линейные комбинаций строк правой матрицы \(B\).

Применим второй взгляд :) Получаем, что строка \(a'\) --- линейная комбинация строк матрицы \(X'\).
Или, другими словами, столбец \(a\) --- линейная комбинация столбцов матрицы \(X\).

\hypertarget{notation}{%
\chapter{Обозначения}\label{notation}}

\(n\) --- количество наблюдений

\(k\) --- количество коэффициентов бета

\(y\) --- вектор зависимой переменной размера \((n\times 1)\)

\(\beta\) --- вектор истинных значений коэффициентов размера \((k \times 1)\)

\(\hat u\) --- остатки модели;

\(\hat u^{CV}\) --- ошибки прогнозов, полученных с помощью кросс-валидации с поочередным выкидыванием отдельных наблюдений;

\(H\) --- матрица-шляпница, \(H = X(X'X)^{-1}X'\)

\bibliography{book.bib,packages.bib}


\end{document}

# МНК и дисперсия {#ols_variance}


Теорема Гаусса-Маркова

Модель $y = X\beta + u$ оценивается с помощью МНК.


TODO: дописать оставшуюся часть теоремы и доказательства


Доказательство эффективности МНК-оценок.


Эффективность МНК-оценок — это реинкарнация теоремы Пифагора. 
Мы докажем, что дисперсия МНК-оценки — это квадрат длины катета, 
дисперсия альтернативной несмещённой оценки — квадрат длины гипотенузы.

Для примера рассмотрим первый коэффициент бета. Доказательство не меняется ни капли, если рассмотреть произвольную линейную комбинацию коэффициентов бета.
У нас есть две оценки, $\hat\beta_1$ и $\hat\beta_1^{alt}$. Обе они линейны по $y$,
следовательно, $\hat\beta_1 = a'y$ и $\hat\beta_1^{alt} = a'_{alt} y$.

Замечаем, что $Var(\hat\beta_1) = \sigma^2 a'a$, и $Var(\hat\beta_1^{alt}) = \sigma^2 a_{alt}'a_{alt}$. То есть дисперсии пропорциональны квадратм длин векторов $a$ и $a^{alt}$. 
Осталось доказать, что вектор $a$ не длиннее вектора $a^{alt}$ :)

Для этого мы докажем, что $a^{alt}$ — это гипотенуза, а $a$ — катет. То есть нужно доказать, что вектор $a - a^{alt}$ перпендикулярен вектору $a$.

Разобъём доказательство перпендикулярности $a$ и $a-a^{alt}$ на два шага:

Шаг 1. Вектор $a - a^{alt}$ перпендикулярен любому столбцу матрицы $X$.

Шаг 2. Вектор $a$ является линейной комбинацией столбцов матрицы $X$.


TODO: здесь картинка!


Приступаем к шагу 1. Обе оценки несмещённые, поэтому для любых $\beta$ должно выполняться:

\[
E(\hat\beta_1) = E(\hat\beta_1^{alt})
\]

Переносим всё в левую сторону:

\[
E((a' - a'_{alt})(X\beta + u)) = 0
\]

Получаем, что для любых $\beta$ должно быть выполнено условие:

\[
(a - a_{alt})'X\beta = 0
\]

Но это возможно только если вектор $(a - a_{alt})'X$ равен нулю. 
Следовательно, вектор $(a - a_{alt})$ перпендикулярен любому столбцу $X$.


Приступаем к шагу 2.

Вспоминаем, что $\hat \beta = (X'X)^{-1}X'y$. Следовательно, нужная строка весов $a'$ — 
это первая строка в матрице $(X'X)^{-1}X'$. 
Замечаем, что выражение имеет вид $A \cdot X'$. 

Вспоминаем из линейной алгебры, что при умножении матриц $AB$ получается матрица $C$, 
на которую можно взглянуть двумя способами. 
Можно считать, что $C$ — это разные линейные комбинации столбцов левой матрицы $A$, 
а можно считать, что $C$ — это разные линейные комбинаций строк правой матрицы $B$.

Применим второй взгляд :) Получаем, что строка $a'$ — линейная комбинация строк матрицы $X'$. 
Или, другими словами, столбец $a$ — линейная комбинация столбцов матрицы $X$.













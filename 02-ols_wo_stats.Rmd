# МНК без статистических предпосылок {#ols_wo_stats}


Определение. 

Кросс-валидация с поочередным выкидыванием отдельных наблюдений.

Leave one out cross validation.

Рассмотрим модель $y=X\beta + u$. 

Оценим модель без первого наблюдения. Получим МНК-оценки $\hat\beta^{(-1)}$.
С помощью этих оценок спрогнозируем первое наблюдение, получим прогноз $\hat y_1^{CV}$ и ошибку прогноза $\hat u_1^{CV}$.

Вернём первое наблюдение в выборку и удалим второе наблюдение. Получим МНК-оценки $\hat\beta^{(-2)}$.
С помощью этих оценок спрогнозируем второе наблюдение, получим прогноз $\hat y_2^{CV}$ и ошибку прогноза $\hat u_2^{CV}$.

Поступим так с каждым наблюдением. На выходе получим вектор кросс-валидационных прогнозов $\hat y^{CV}$ и вектор кросс-валидационных ошибок прогнозов $\hat u^{CV}$.


Теорема. 

Если модель $y=X\beta + u$ оценивается с помощью МНК и проводится кросс-валидации с поочередным выкидыванием отдельных наблюдений, то:

\[
\hat u_i = (1 - H_{ii}) \cdot \hat u_i^{CV},
\]
где $H$ — матрица-шляпница $H = X(X'X)^{-1}X'$, $\hat u$ — остатки регрессии, а $\hat u^{CV}$ — кросс-валидационные ошибки прогнозов.

Доказательство. 

Оценим модель без последнего наблюдения, $\hat y^{-} = X^{-} \hat\beta^{-}$. 

Создадим вектор $y^{*}$, который будет отличаться от $y$ только последним, $n$-м элементом:
вместо настоящего $y_n$ там будет стоять прогноз по модели без последнего наблюдения $\hat y^{-}_n$.

Раз уж мы добавили новую точку лежащую ровно на выборочной регрессии, то при оценки модели
$\hat y^* = X \hat \beta^*$ мы получим в точности старые оценки $\hat \beta^* = \hat \beta^-$. 
Следовательно, и прогнозы эти две модели дают одинаковые, $\hat y_i^* = \hat y_i^-$.

А теперь посмотрим на последний элемент вектора $v = H (y^* - y)$.

С одной стороны, он равен последней строке матрицы $H$ умножить на вектор $(y^* - y)$. 
В векторе $(y^* - y)$ только последний элемент ненулевой, поэтому $v_n = H_{nn} (\hat y^{-}_n - y_n)$.

С другой стороны, мы можем раскрыть скобки, и заметить, что $v = Hy^* - Hy$. 
И окажется, что $v_n = \hat y_n^* - \hat y_n = \hat y_n^- - \hat y_n$.

Отсюда
\[
 \hat y_n^- - \hat y_n = H_{nn} (\hat y_n^- - y_n)
\]

Приводим подобные слагаемые и добавляем слева и справа $y_n$, получаем как раз то, что нужно:
\[
y_n - \hat y_n   = (1 - H_{ii}) (y_n - \hat y_n^- )
\]


